{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSuV2tFKEk5Z"
      },
      "source": [
        "# TASK 2 ⏰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzhKeqq571ia"
      },
      "source": [
        "### Install Apache Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9_2wjgO2jvN",
        "outputId": "2c7aedee-57fe-4207-992b-d58bf38c975f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPhoFLS17_PZ"
      },
      "source": [
        "### Setup Apache Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ChRP6j8F1_SK"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"MySparkSession\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd5AT2ya8CMp"
      },
      "source": [
        "### Data Frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rqc6xfNM3S3X"
      },
      "outputs": [],
      "source": [
        "#Data Frame is useful for analyzing data, and using sql like queries\n",
        "df = spark.read.csv(\"customer_churn.csv\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a3zQPhZ3TCH",
        "outputId": "658c1df7-e574-4bcc-9463-c098600608d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- CustomerID: integer (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- Subscription_Length_Months: integer (nullable = true)\n",
            " |-- Watch_Time_Hours: double (nullable = true)\n",
            " |-- Number_of_Logins: integer (nullable = true)\n",
            " |-- Preferred_Content_Type: string (nullable = true)\n",
            " |-- Membership_Type: string (nullable = true)\n",
            " |-- Payment_Method: string (nullable = true)\n",
            " |-- Payment_Issues: integer (nullable = true)\n",
            " |-- Number_of_Complaints: integer (nullable = true)\n",
            " |-- Resolution_Time_Days: integer (nullable = true)\n",
            " |-- Churn: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YET3W8SE-igk",
        "outputId": "94c643b6-a18a-46b8-8bb5-97bceafd2efd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EaC8hts8Ejo"
      },
      "source": [
        "### RDD (Resilient Distributed Dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0Fcia7D7nOB"
      },
      "source": [
        "### 1. Initialize PySpark and Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ikAym39-3TNF"
      },
      "outputs": [],
      "source": [
        "#RDD is more useful for applying functional transformations like map, reduce, etc.\n",
        "rdd = spark.sparkContext.textFile(\"customer_churn.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0tO7faH7k_l"
      },
      "source": [
        "### 2. Perform the MapReduce Job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9m-ie-MI3Tck"
      },
      "outputs": [],
      "source": [
        "# Map: Split lines into words and assign an initial count of 1\n",
        "# flatMap(lambda line: line.split(\" \")) will split each line into words\n",
        "# map(lambda word: (word, 1) maps each word to a (word, 1) tuple\n",
        "words_rdd = rdd.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1))\n",
        "\n",
        "# Reduce: Sum counts for each word\n",
        "word_counts = words_rdd.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Collect and show the results\n",
        "#for word, count in word_counts.collect():\n",
        "    #print(word, count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7lz7-RW8ZDW",
        "outputId": "8fd2eb27-07de-447c-8b52-d77cccda39c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Shows,Standard,Credit', 32), ('Shows,Basic,Bank', 25), ('Shows,Standard,Bank', 25), ('Shows,Premium,Bank', 23), ('Shows,Premium,Credit', 23), ('Shows,Basic,Credit', 21), ('Transfer,0,0,26,1', 4), ('Card,0,7,21,0', 3), ('Transfer,0,7,18,0', 3), ('Card,0,7,4,0', 3)]\n"
          ]
        }
      ],
      "source": [
        "#Show top 10 words\n",
        "sorted_word_counts = word_counts.sortBy(lambda x: x[1], ascending=False)\n",
        "print(sorted_word_counts.take(10))  # Show top 10 words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b6lt1kg991u"
      },
      "source": [
        "\n",
        "* **Volume:**  We can see there are 1000 entries using df.count()\n",
        "\n",
        "* **Velocity:** The data is batch processed. Meaning that the data existed already before some time, and was collected into a cluster to be analysed.\n",
        "\n",
        "* **Variety:** We can see the structure using the Data Frame and doing\n",
        "df.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXcF3XRfEhWt"
      },
      "source": [
        "# TASK 3 ⏰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM7QPrK6-7Q3"
      },
      "source": [
        "## • What advantages does Spark offer over traditional MapReduce?\n",
        "Sparks map reduce is significantly quicker than the traditional map reduce through the use of RDD with lambda functions and a built-in support for statistical computations.\n",
        "\n",
        "It also uses lazy evaluations which means that it uses a recorded lineage of operations. Execution occurs only when an action (collet, reduce) is triggered.\n",
        "\n",
        "RDDs can be persisted (cached) in memory or disk for reuse, enhancing performance for iterative computations\n",
        "\n",
        "RDDs allow batch processing of multiple data items simultaneously, making them\n",
        "efficient for parallel data processing.\n",
        "## • How does Spark's in-memory computation improve performance?\n",
        "RDD's improve the speed of map reduce thanks to a few things. One of its key features is its In-Memory computation which is known for being quicker than a traditional disk I/O.\n",
        "\n",
        "\n",
        "### MapReduce Approach\n",
        "Reads data from HDFS → Maps → Writes to disk → Shuffles → Reduces → Writes final output to HDFS.\n",
        "\n",
        "Multiple disk I/O operations slow down performance.\n",
        "\n",
        "### Spark Approach\n",
        "Loads data into memory → Processes transformations in-memory → Writes final output to disk once.\n",
        "\n",
        "No repeated disk writes, leading to 10-100x speedup."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eSZyeCOEQ8o"
      },
      "source": [
        "# TASK 5 ⏰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z0qUPjvFQoT"
      },
      "source": [
        "| Feature             | **Apache Spark**                                                       | **Apache Hive**                                                    |\n",
        "|---------------------|------------------------------------------------------------------------|--------------------------------------------------------------------|\n",
        "| **Structured Data**  | Efficient with DataFrames and SQL-like queries (Spark SQL). Optimized for in-memory processing. | Designed for structured data with SQL-based querying. Runs on top of Hadoop. |\n",
        "| **Unstructured Data**| Can process unstructured data (e.g., text, images) using RDDs and ML libraries. | Primarily for structured data but can handle semi-structured (JSON, XML) with extra parsing. |\n",
        "| **Processing Speed** | In-memory computation makes it much faster for iterative tasks.         | Uses disk-based processing (MapReduce), making it slower.         |\n",
        "| **Use Case**         | Best for real-time analytics, machine learning, and complex transformations. | Best for batch processing, ETL, and data warehousing.             |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQQXDpKxFUsv"
      },
      "source": [
        "Spark is more flexible for unstructured data, while Hive is optimized for structured queries."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

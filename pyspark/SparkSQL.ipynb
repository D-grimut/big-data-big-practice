{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KrVxFDeOnlG9"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2b1MNRToR8J",
        "outputId": "49815ec8-eed2-445e-8f56-53c87329c0f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+---+\n",
            "|   Name|Age|\n",
            "+-------+---+\n",
            "|  Alice| 25|\n",
            "|    Bob| 30|\n",
            "|Charlie| 35|\n",
            "+-------+---+\n",
            "\n",
            "+-------+---+\n",
            "|   Name|Age|\n",
            "+-------+---+\n",
            "|    Bob| 30|\n",
            "|Charlie| 35|\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# create data array (rows), and columns name array\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "\n",
        "# Creates a temporay session-scoped logical table from a DataFrame - this creates an IMMUTABLE view of the table, since we don't have hive (or another db attached), the table spark generates is virtual and immuatble\n",
        "# thus, we CANNOT insert stuff into the table in this case.\n",
        "df.createOrReplaceTempView(\"sqltable\")\n",
        "\n",
        "# runing an SQL querry - produces a new dataframe - notice how we querry from the temp table sqltable we created\n",
        "res = spark.sql(\"select * from sqltable where age > 25\")\n",
        "res.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIwHTpGlpcYv",
        "outputId": "7cd08091-1ce0-47ba-b94b-1f56fca5eaed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----------+-------+------+\n",
            "|age|department|   name|salary|\n",
            "+---+----------+-------+------+\n",
            "| 25|        HR|  Alice| 50000|\n",
            "| 30|        IT|    Bob| 70000|\n",
            "| 35|   Finance|Charlie| 80000|\n",
            "| 40|        IT|  David| 90000|\n",
            "| 45|   Finance|    Eve|100000|\n",
            "+---+----------+-------+------+\n",
            "\n",
            "+---+----------+-------+------+\n",
            "|age|department|   name|salary|\n",
            "+---+----------+-------+------+\n",
            "| 35|   Finance|Charlie| 80000|\n",
            "| 40|        IT|  David| 90000|\n",
            "| 45|   Finance|    Eve|100000|\n",
            "+---+----------+-------+------+\n",
            "\n",
            "+----------+--------------+----------+\n",
            "|department|employee_count|avg_salary|\n",
            "+----------+--------------+----------+\n",
            "|   Finance|             2|   90000.0|\n",
            "|        IT|             2|   80000.0|\n",
            "|        HR|             1|   50000.0|\n",
            "+----------+--------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# JSON style data\n",
        "data = [\n",
        " {\"name\": \"Alice\", \"age\": 25, \"department\": \"HR\", \"salary\": 50000},\n",
        " {\"name\": \"Bob\", \"age\": 30, \"department\": \"IT\", \"salary\": 70000},\n",
        " {\"name\": \"Charlie\", \"age\": 35, \"department\": \"Finance\", \"salary\": 80000},\n",
        " {\"name\": \"David\", \"age\": 40, \"department\": \"IT\", \"salary\": 90000},\n",
        " {\"name\": \"Eve\", \"age\": 45, \"department\": \"Finance\", \"salary\": 100000},\n",
        "]\n",
        "\n",
        "df_json = spark.createDataFrame(data)\n",
        "df_json.show()\n",
        "\n",
        "df_json.createOrReplaceTempView(\"jsontable\")\n",
        "spark.sql(\"select * from jsontable where salary >= 80000\").show()\n",
        "\n",
        "# ex with a bigger querry\n",
        "query = \"\"\"\n",
        " SELECT department, COUNT(*) AS employee_count, AVG(salary) AS avg_salary\n",
        " FROM jsontable\n",
        " WHERE age > 20\n",
        " GROUP BY department\n",
        " ORDER BY avg_salary DESC\n",
        "\"\"\"\n",
        "\n",
        "result = spark.sql(query)\n",
        "result.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFxIXGNuqjtj",
        "outputId": "c8b720cb-abdc-4c66-9b13-8ef7cce3e15f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-------+-------+------+\n",
            "| ID|   Name|   Dept| Bread|\n",
            "+---+-------+-------+------+\n",
            "|  1|  Alice|     HR| 50000|\n",
            "|  2|    Bob|     IT| 70000|\n",
            "|  3|Charlie|Finance| 80000|\n",
            "|  4|  David|     IT| 90000|\n",
            "|  5|    Eve|Finance|100000|\n",
            "+---+-------+-------+------+\n",
            "\n",
            "+------+-------+----------+------+\n",
            "|emp_id|   name|department|salary|\n",
            "+------+-------+----------+------+\n",
            "|     5|    Eve|   Finance|100000|\n",
            "|     3|Charlie|   Finance| 80000|\n",
            "|     2|    Bob|        IT| 70000|\n",
            "|     4|  David|        IT| 90000|\n",
            "|     1|  Alice|        HR| 50000|\n",
            "|     6|  Frank| Marketing|110000|\n",
            "|     7|  Grace|        HR| 60000|\n",
            "+------+-------+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "employees_data = [\n",
        " (1, \"Alice\", \"HR\"),\n",
        " (2, \"Bob\", \"IT\"),\n",
        " (3, \"Charlie\", \"Finance\"),\n",
        " (4, \"David\", \"IT\"),\n",
        " (5, \"Eve\", \"Finance\"),\n",
        "]\n",
        "\n",
        "employees_columns = [\"emp_id\", \"name\", \"department\"]\n",
        "\n",
        "emp_df = spark.createDataFrame(employees_data, employees_columns)\n",
        "\n",
        "# again, we do this to create a temporary SQL table\n",
        "emp_df.createOrReplaceTempView(\"employees\")\n",
        "\n",
        "salaries_data = [\n",
        " (1, 50000),\n",
        " (2, 70000),\n",
        " (3, 80000),\n",
        " (4, 90000),\n",
        " (5, 100000),\n",
        "]\n",
        "\n",
        "salaries_columns = [\"emp_id\", \"salary\"]\n",
        "salary_df = spark.createDataFrame(salaries_data, salaries_columns)\n",
        "salary_df.createOrReplaceTempView(\"salary\")\n",
        "\n",
        "# SQL refresh - we are selecting from both tables, but we are merging both first\n",
        "# then we look at whatever the data is in the merge, and select from there.\n",
        "\n",
        "# The 'from' here tells SQL what table to start with, what is the beggining dataset\n",
        "# to work with, and then the 'join' joins that dataset with the salary table.\n",
        "q = \"\"\"\n",
        "select e.emp_id as ID, e.name as Name, e.department as Dept, s.salary as Bread\n",
        "from employees e\n",
        "join salary s on e.emp_id = s.emp_id\n",
        "\"\"\"\n",
        "\n",
        "# joining both tables, this produces another table which SPARK saves as a DF.\n",
        "employees_joined = spark.sql(q)\n",
        "employees_joined.show()\n",
        "\n",
        "new_employees_data = [\n",
        " (6, \"Frank\", \"Marketing\", 110000),\n",
        " (7, \"Grace\", \"HR\", 60000),\n",
        "]\n",
        "\n",
        "new_employees_columns = [\"emp_id\", \"name\", \"department\", \"salary\"]\n",
        "\n",
        "new_emp_df = spark.createDataFrame(new_employees_data, new_employees_columns)\n",
        "new_emp_df.createOrReplaceTempView(\"newemps\")\n",
        "\n",
        "# recall: union appends rows to the table (vertical), \"join\" appends columns (horizontal).\n",
        "# in the nested querry, we are creating a new talbe, which we call \"joined\" - from now on,\n",
        "# to referance the values inside this new table we need to do \"joined.<col name>\"\n",
        "q_union = \"\"\"\n",
        "select joined.emp_id as ID, joined.name as Name, joined.department as Dept, joined.salary as Bread\n",
        "  from (select e.emp_id, e.name, e.department, s.salary\n",
        "  from employees e\n",
        "  join salary s on e.emp_id = s.emp_id) joined\n",
        "union\n",
        "select ne.emp_id, ne.name, ne.department, ne.salary\n",
        "from newemps ne\n",
        "\"\"\"\n",
        "spark.sql(q_union).show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
